{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will show how to apply our methodoly for one of the considered dataset - Anger dataset (should be located in the 'data' folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset references:**\n",
    "Semeval-2018 Task 1: Affect in Tweets. Saif M. Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. In Proceedings of International Workshop on Semantic Evaluation (SemEval-2018), New Orleans, LA, USA, June 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this packages for embeddings can be uploaded here or in embeddings_and_lexicons.py (commented section)\n",
    "# torchmoji \n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding \n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "# roBERTa model \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "# Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# SBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# BERT\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# import our functions\n",
    "sys.path.append('./code')\n",
    "from preprocessing import *\n",
    "from embeddings_and_lexicons import *\n",
    "from wknn_eval import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload data and perform preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = './data/SemEval2018-Task1-all-data/English_EI-oc/training/EI-oc-En-anger-train.txt'\n",
    "file_dev = './data/SemEval2018-Task1-all-data/English_EI-oc/development/2018-EI-oc-En-anger-dev.txt'\n",
    "file_test = './data/SemEval2018-Task1-all-data/English_EI-oc/test-gold/2018-EI-oc-En-anger-test-gold.txt'\n",
    "\n",
    "columns = ['ID', 'Tweet', 'Affect Dimension', 'Intensity Class']\n",
    "sep = '\\t'\n",
    "\n",
    "train_data, dev_data, eval_data, test_data = upload_datasets(file_train, file_dev, file_test, columns, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "      <th>Cleaned_tweet</th>\n",
       "      <th>Cleaned_tweet_wt_stopwords</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-10264</td>\n",
       "      <td>@xandraaa5 @amayaallyn6 shut up hashtags are c...</td>\n",
       "      <td>anger</td>\n",
       "      <td>2: moderate amount of anger can be inferred</td>\n",
       "      <td>$MENTION$ $MENTION$ shut up hashtags are cool ...</td>\n",
       "      <td>$MENTION$ $MENTION$ shut hashtags cool offended</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-10072</td>\n",
       "      <td>it makes me so fucking irate jesus. nobody is ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>3: high amount of anger can be inferred</td>\n",
       "      <td>it makes me so fucking irate jesus. nobody is ...</td>\n",
       "      <td>makes fucking irate jesus. nobody calling ppl ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-11383</td>\n",
       "      <td>Lol Adam the Bull with his fake outrage...</td>\n",
       "      <td>anger</td>\n",
       "      <td>1: low amount of anger can be inferred</td>\n",
       "      <td>Lol Adam the Bull with his fake outrage...</td>\n",
       "      <td>Lol Adam Bull fake outrage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  \\\n",
       "0  2017-En-10264  @xandraaa5 @amayaallyn6 shut up hashtags are c...   \n",
       "1  2017-En-10072  it makes me so fucking irate jesus. nobody is ...   \n",
       "2  2017-En-11383         Lol Adam the Bull with his fake outrage...   \n",
       "\n",
       "  Affect Dimension                              Intensity Class  \\\n",
       "0            anger  2: moderate amount of anger can be inferred   \n",
       "1            anger      3: high amount of anger can be inferred   \n",
       "2            anger       1: low amount of anger can be inferred   \n",
       "\n",
       "                                       Cleaned_tweet  \\\n",
       "0  $MENTION$ $MENTION$ shut up hashtags are cool ...   \n",
       "1  it makes me so fucking irate jesus. nobody is ...   \n",
       "2         Lol Adam the Bull with his fake outrage...   \n",
       "\n",
       "                          Cleaned_tweet_wt_stopwords  Class  \n",
       "0    $MENTION$ $MENTION$ shut hashtags cool offended      2  \n",
       "1  makes fucking irate jesus. nobody calling ppl ...      3  \n",
       "2                      Lol Adam Bull fake outrage...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read lexicons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaaaaah</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaah</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Valence  Arousal  Dominance\n",
       "0  aaaaaaah    0.479    0.606      0.291\n",
       "1     aaaah    0.520    0.636      0.282\n",
       "2  aardvark    0.427    0.490      0.437"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read VAD lexicon\n",
    "\n",
    "vad_file = './lexica/NRC-VAD/NRC-VAD-Lexicon.txt'\n",
    "vad = pd.read_csv(vad_file, sep=\"\\t\")\n",
    "vad.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abacus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
       "0     NaN      0             0        0     0    0         0         0   \n",
       "1   aback      0             0        0     0    0         0         0   \n",
       "2  abacus      0             0        0     0    0         0         0   \n",
       "\n",
       "   sadness  surprise  trust  \n",
       "0        0         0      0  \n",
       "1        0         0      0  \n",
       "2        0         0      1  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EMOLEX lexicon\n",
    "\n",
    "emolex_lexicon = pd.read_csv('./lexica/NRC-Emotion/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', sep=\"\\t\", header=None)\n",
    "emolex_lexicon.columns = [\"Word\", \"Emotion\", \"Availability\"]\n",
    "\n",
    "# Transform EMOLEX to DataFrame form\n",
    "\n",
    "emolex = emolex_lexicon.pivot(columns='Emotion',values='Availability',index='Word').reset_index()\n",
    "emolex.columns = ['Word', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
    "emolex.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaaaaaah</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaah</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       term  anger   fear    joy  sadness\n",
       "0      TRUE    0.0  0.000  0.328      0.0\n",
       "1  aaaaaaah    0.0  0.344  0.000      0.0\n",
       "2     aaaah    0.0  0.234  0.000      0.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Affect Intensity lexicon\n",
    "\n",
    "ai_lexicon = pd.read_csv('./lexica/NRC-Affect-Intensity/NRC-AffectIntensity-Lexicon.txt', sep=\"\\t\")\n",
    "ai_lexicon.head()\n",
    "\n",
    "# Transform AI to DataFrame form\n",
    "\n",
    "ai = ai_lexicon.pivot(columns='AffectDimension',values='score',index='term').reset_index()\n",
    "ai.columns = ['term', 'anger', 'fear', 'joy', 'sadness']\n",
    "ai = ai.fillna(0)\n",
    "ai.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Valence Mean</th>\n",
       "      <th>Valence SD</th>\n",
       "      <th>Arousal Mean</th>\n",
       "      <th>Arousal SD</th>\n",
       "      <th>Dominance Mean</th>\n",
       "      <th>Dominance SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grin</td>\n",
       "      <td>7.40</td>\n",
       "      <td>1.87</td>\n",
       "      <td>5.27</td>\n",
       "      <td>2.64</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>honest</td>\n",
       "      <td>7.70</td>\n",
       "      <td>1.43</td>\n",
       "      <td>5.32</td>\n",
       "      <td>1.92</td>\n",
       "      <td>6.24</td>\n",
       "      <td>2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gripe</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1.56</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.19</td>\n",
       "      <td>4.67</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Valence Mean  Valence SD  Arousal Mean  Arousal SD  Dominance Mean  \\\n",
       "0    grin          7.40        1.87          5.27        2.64            6.00   \n",
       "1  honest          7.70        1.43          5.32        1.92            6.24   \n",
       "2   gripe          3.14        1.56          5.00        2.19            4.67   \n",
       "\n",
       "   Dominance SD  \n",
       "0          1.86  \n",
       "1          2.13  \n",
       "2          1.79  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANEW lexicon\n",
    "\n",
    "anew = pd.read_csv('./lexica/ANEW/all.csv')\n",
    "anew.rename(columns={'Description': 'Word'}, inplace=True)\n",
    "del anew['Word No.']\n",
    "del anew['Word Frequency']\n",
    "anew.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>V.Mean.Sum</th>\n",
       "      <th>V.SD.Sum</th>\n",
       "      <th>V.Rat.Sum</th>\n",
       "      <th>A.Mean.Sum</th>\n",
       "      <th>A.SD.Sum</th>\n",
       "      <th>A.Rat.Sum</th>\n",
       "      <th>D.Mean.Sum</th>\n",
       "      <th>D.SD.Sum</th>\n",
       "      <th>D.Rat.Sum</th>\n",
       "      <th>...</th>\n",
       "      <th>A.Rat.L</th>\n",
       "      <th>A.Mean.H</th>\n",
       "      <th>A.SD.H</th>\n",
       "      <th>A.Rat.H</th>\n",
       "      <th>D.Mean.L</th>\n",
       "      <th>D.SD.L</th>\n",
       "      <th>D.Rat.L</th>\n",
       "      <th>D.Mean.H</th>\n",
       "      <th>D.SD.H</th>\n",
       "      <th>D.Rat.H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.21</td>\n",
       "      <td>19</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.40</td>\n",
       "      <td>22</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.75</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.29</td>\n",
       "      <td>11</td>\n",
       "      <td>4.12</td>\n",
       "      <td>1.64</td>\n",
       "      <td>8</td>\n",
       "      <td>4.43</td>\n",
       "      <td>1.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abalone</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.59</td>\n",
       "      <td>20</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.90</td>\n",
       "      <td>20</td>\n",
       "      <td>4.95</td>\n",
       "      <td>1.79</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.92</td>\n",
       "      <td>8</td>\n",
       "      <td>5.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>11</td>\n",
       "      <td>4.36</td>\n",
       "      <td>1.03</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.54</td>\n",
       "      <td>19</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.43</td>\n",
       "      <td>22</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.50</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.09</td>\n",
       "      <td>13</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  V.Mean.Sum  V.SD.Sum  V.Rat.Sum  A.Mean.Sum  A.SD.Sum  A.Rat.Sum  \\\n",
       "1  aardvark        6.26      2.21         19        2.41      1.40         22   \n",
       "2   abalone        5.30      1.59         20        2.65      1.90         20   \n",
       "3   abandon        2.84      1.54         19        3.73      2.43         22   \n",
       "\n",
       "   D.Mean.Sum  D.SD.Sum  D.Rat.Sum  ...  A.Rat.L  A.Mean.H  A.SD.H  A.Rat.H  \\\n",
       "1        4.27      1.75         15  ...       11      2.55    1.29       11   \n",
       "2        4.95      1.79         22  ...       12      2.38    1.92        8   \n",
       "3        3.32      2.50         22  ...       11      3.82    2.14       11   \n",
       "\n",
       "   D.Mean.L  D.SD.L  D.Rat.L  D.Mean.H  D.SD.H  D.Rat.H  \n",
       "1      4.12    1.64        8      4.43    1.99        7  \n",
       "2      5.55    2.21       11      4.36    1.03       11  \n",
       "3      2.77    2.09       13      4.11    2.93        9  \n",
       "\n",
       "[3 rows x 64 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warriner lexicon\n",
    "\n",
    "warriner = pd.read_csv('./lexica/AffectiveNorms/Ratings_Warriner_et_al.csv', index_col=0)\n",
    "warriner.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lexicons in the list\n",
    "\n",
    "lexicons_data = [[vad, 'Word', 3, 0, 1], [emolex, 'Word', 10, 0, 1], [ai, 'term', 4, 0, 1],\n",
    "                 [anew, 'Word', 6, 0, 9], [warriner, 'Word', 63, 0, 1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply lexicons to the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = lexicons_data[0]\n",
    "eval_data['Vector_vad'] = eval_data[Raw_tweet].apply(lambda x: np.mean([get_lexicon_scores(i, lex[0], lex[1], lex[2]) \n",
    "                                                                        for i in x.split(' ')], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = lexicons_data[1]\n",
    "eval_data['Vector_emolex'] = eval_data[Raw_tweet].apply(lambda x: np.mean([get_lexicon_scores(i, lex[0], lex[1], lex[2]) \n",
    "                                                                        for i in x.split(' ')], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = lexicons_data[2]\n",
    "eval_data['Vector_ai'] = eval_data[Raw_tweet].apply(lambda x: np.mean([get_lexicon_scores(i, lex[0], lex[1], lex[2]) \n",
    "                                                                        for i in x.split(' ')], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = lexicons_data[3]\n",
    "eval_data['Vector_anew'] = eval_data[Raw_tweet].apply(lambda x: np.mean([get_lexicon_scores(i, lex[0], lex[1], lex[2]) \n",
    "                                                                        for i in x.split(' ')], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = lexicons_data[4]\n",
    "eval_data['Vector_warriner'] = eval_data[Raw_tweet].apply(lambda x: np.mean([get_lexicon_scores(i, lex[0], lex[1], lex[2]) \n",
    "                                                                        for i in x.split(' ')], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combination of all lexicons\n",
    "eval_data['Vector_all_lexicons'] = eval_data.apply(lambda x: x['Vector_vad'].tolist()+x['Vector_emolex'].tolist()+\n",
    "                                   x['Vector_ai'].tolist()+x['Vector_anew'].tolist()+x['Vector_warriner'].tolist(), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply embedding methods on the evaluation (train+dev) data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_tweet = 'Tweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roBERTa\n",
    "\n",
    "# path to the pre-loaded roBERTa model \n",
    "MODEL_path_roberta = \"./model/twitter-roberta-base-emotion\"\n",
    "# upload tokenizer and model\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(MODEL_path_roberta)\n",
    "model_roberta = TFAutoModel.from_pretrained(MODEL_path_roberta)\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in [Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']:\n",
    "    eval_data['Vector_roBERTa_'+column] = eval_data[column].apply(lambda x: \n",
    "                                            get_vector_roberta(x, tokenizer_roberta, model_roberta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepMoji\n",
    "\n",
    "# If dataset is big, it's better to split it\n",
    "for column in [Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']:\n",
    "    eval_data['Vector_DeepMoji_'+column] = None\n",
    "    for i in range(50):\n",
    "        ind = list(range(i*int(len(eval_data[column])/50),(i+1)*int(len(eval_data[column])/50)))\n",
    "        eval_data['Vector_DeepMoji_'+column].iloc[ind] = get_vectors_deepmoji(eval_data[column].iloc[ind]) \n",
    "\n",
    "# If dataset is small, you can use this:\n",
    "#for column in [Raw_tweet, 'Cleaned_tweet','Cleaned_tweet_wt_stopwords']:\n",
    "#    eval_data['Vector_DeepMoji_'+column] = get_vectors_deepmoji(eval_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "# load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# upload BERT model \n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in [Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']:\n",
    "    eval_data['Vector_BERT_'+column] = eval_data[column].apply(lambda x: \n",
    "                                                      get_vector_bert(x, tokenizer_bert, model_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sBERT\n",
    "\n",
    "# upload Sentence-BERT model from the 'sentence_transformers' package \n",
    "model_sbert = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in [Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']:\n",
    "    eval_data['Vector_sBERT_'+column] = eval_data[column].apply(lambda x: get_vector_sbert(x, model_sbert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE \n",
    "\n",
    "# upload the big Universal Sentence Encoder model from HTTPS domain \n",
    "model_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in [Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']:\n",
    "    eval_data['Vector_USE_'+column] = eval_data[column].apply(lambda x: get_vector_use(x, model_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "\n",
    "# path to the pre-loaded Word2Vec model \n",
    "w2v_path = './model/GoogleNews-vectors-negative300.bin'\n",
    "# upload model\n",
    "model_w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in [Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']:\n",
    "    eval_data['Vector_Word2Vec_'+column] = eval_data[column].apply(lambda x: get_vector_w2v(x, model_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Append lexicons to embedding vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding in ['roBERTa', 'DeepMoji', 'BERT', 'sBERT', 'USE', 'Word2Vec']:\n",
    "    eval_data[\"Vector_\"+embedding+\"_vad\"] = eval_data.apply(lambda x: append_lexicon_scores(x['Tweet'], x[\"Vector_\"+embedding+\"_\"+Raw_tweet], lexicons_data[0]), axis=1)\n",
    "    eval_data[\"Vector_\"+embedding+\"_emolex\"] = eval_data.apply(lambda x: append_lexicon_scores(x['Tweet'], x[\"Vector_\"+embedding+\"_\"+Raw_tweet], lexicons_data[1]), axis=1)\n",
    "    eval_data[\"Vector_\"+embedding+\"_ai\"] = eval_data.apply(lambda x: append_lexicon_scores(x['Tweet'], x[\"Vector_\"+embedding+\"_\"+Raw_tweet], lexicons_data[2]), axis=1)\n",
    "    eval_data[\"Vector_\"+embedding+\"_anew\"] = eval_data.apply(lambda x: append_lexicon_scores(x['Tweet'], x[\"Vector_\"+embedding+\"_\"+Raw_tweet], lexicons_data[3]), axis=1)\n",
    "    eval_data[\"Vector_\"+embedding+\"_warriner\"] = eval_data.apply(lambda x: append_lexicon_scores(x['Tweet'], x[\"Vector_\"+embedding+\"_\"+Raw_tweet], lexicons_data[4]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply cross-validation for wkNN to evaluate k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_fold = 5\n",
    "\n",
    "k_list = [5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_raw_roBERTa = []\n",
    "pcc_clean_roBERTa = []\n",
    "pcc_wtstop_roBERTa = []\n",
    "\n",
    "for k in k_list:\n",
    "    pcc_raw_roBERTa.append(cross_validation_ensemble_knn(eval_data, ['Vector_roBERTa_'+Raw_tweet], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_clean_roBERTa.append(cross_validation_ensemble_knn(eval_data, ['Vector_roBERTa_Cleaned_tweet'], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_wtstop_roBERTa.append(cross_validation_ensemble_knn(eval_data, ['Vector_roBERTa_Cleaned_tweet_wt_stopwords'], 'Class', K_fold, [k], 'labels', 'pcc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest PCC score:  0.6637482135603178  with k =  23\n",
      "The highest PCC score:  0.6579449484161686  with k =  13\n",
      "The highest PCC score:  0.6436266193028504  with k =  23\n"
     ]
    }
   ],
   "source": [
    "for array in [pcc_raw_roBERTa, pcc_clean_roBERTa, pcc_wtstop_roBERTa]:\n",
    "    print('The highest PCC score: ', max(array), ' with k = ', k_list[array.index(max(array))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result for roBERTa embedding is PCC = 0.6637, for the raw tweets with k=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_raw_DeepMoji = []\n",
    "pcc_clean_DeepMoji = []\n",
    "pcc_wtstop_DeepMoji = []\n",
    "\n",
    "for k in k_list:\n",
    "    pcc_raw_DeepMoji.append(cross_validation_ensemble_knn(eval_data, ['Vector_DeepMoji_'+Raw_tweet], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_clean_DeepMoji.append(cross_validation_ensemble_knn(eval_data, ['Vector_DeepMoji_Cleaned_tweet'], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_wtstop_DeepMoji.append(cross_validation_ensemble_knn(eval_data, ['Vector_DeepMoji_Cleaned_tweet_wt_stopwords'], 'Class', K_fold, [k], 'labels', 'pcc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest PCC score:  0.559513358149148  with k =  21\n",
      "The highest PCC score:  0.572431579806779  with k =  25\n",
      "The highest PCC score:  0.5300392319317812  with k =  17\n"
     ]
    }
   ],
   "source": [
    "for array in [pcc_raw_DeepMoji, pcc_clean_DeepMoji, pcc_wtstop_DeepMoji]:\n",
    "    print('The highest PCC score: ', max(array), ' with k = ', k_list[array.index(max(array))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result for DeepMoji embedding is PCC = 0.5724, for the cleaned tweets with k=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_raw_BERT = []\n",
    "pcc_clean_BERT = []\n",
    "pcc_wtstop_BERT = []\n",
    "\n",
    "for k in k_list:\n",
    "    pcc_raw_BERT.append(cross_validation_ensemble_knn(eval_data, ['Vector_BERT_'+Raw_tweet], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_clean_BERT.append(cross_validation_ensemble_knn(eval_data, ['Vector_BERT_Cleaned_tweet'], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_wtstop_BERT.append(cross_validation_ensemble_knn(eval_data, ['Vector_BERT_Cleaned_tweet_wt_stopwords'], 'Class', K_fold, [k], 'labels', 'pcc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest PCC score:  0.4271145112625013  with k =  25\n",
      "The highest PCC score:  0.41817036049973316  with k =  23\n",
      "The highest PCC score:  0.4231505798100299  with k =  23\n"
     ]
    }
   ],
   "source": [
    "for array in [pcc_raw_BERT, pcc_clean_BERT, pcc_wtstop_BERT]:\n",
    "    print('The highest PCC score: ', max(array), ' with k = ', k_list[array.index(max(array))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result for BERT embedding is PCC = 0.4271, for the raw tweets with k=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_raw_sBERT = []\n",
    "pcc_clean_sBERT = []\n",
    "pcc_wtstop_sBERT = []\n",
    "\n",
    "for k in k_list:\n",
    "    pcc_raw_sBERT.append(cross_validation_ensemble_knn(eval_data, ['Vector_sBERT_'+Raw_tweet], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_clean_sBERT.append(cross_validation_ensemble_knn(eval_data, ['Vector_sBERT_Cleaned_tweet'], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_wtstop_sBERT.append(cross_validation_ensemble_knn(eval_data, ['Vector_sBERT_Cleaned_tweet_wt_stopwords'], 'Class', K_fold, [k], 'labels', 'pcc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest PCC score:  0.4921208438658003  with k =  19\n",
      "The highest PCC score:  0.47359940406435647  with k =  17\n",
      "The highest PCC score:  0.4815516848012857  with k =  19\n"
     ]
    }
   ],
   "source": [
    "for array in [pcc_raw_sBERT, pcc_clean_sBERT, pcc_wtstop_sBERT]:\n",
    "    print('The highest PCC score: ', max(array), ' with k = ', k_list[array.index(max(array))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result for sBERT embedding is PCC = 0.4921, for the raw tweets with k=19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_raw_USE = []\n",
    "pcc_clean_USE = []\n",
    "pcc_wtstop_USE = []\n",
    "\n",
    "for k in k_list:\n",
    "    pcc_raw_USE.append(cross_validation_ensemble_knn(eval_data, ['Vector_USE_'+Raw_tweet], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_clean_USE.append(cross_validation_ensemble_knn(eval_data, ['Vector_USE_Cleaned_tweet'], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_wtstop_USE.append(cross_validation_ensemble_knn(eval_data, ['Vector_USE_Cleaned_tweet_wt_stopwords'], 'Class', K_fold, [k], 'labels', 'pcc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest PCC score:  0.5041695261922098  with k =  21\n",
      "The highest PCC score:  0.5141543858558131  with k =  25\n",
      "The highest PCC score:  0.517907031254379  with k =  21\n"
     ]
    }
   ],
   "source": [
    "for array in [pcc_raw_USE, pcc_clean_USE, pcc_wtstop_USE]:\n",
    "    print('The highest PCC score: ', max(array), ' with k = ', k_list[array.index(max(array))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result for USE embedding is PCC = 0.5179, for the cleaned tweets without stop words with k=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcc_raw_Word2Vec = []\n",
    "pcc_clean_Word2Vec = []\n",
    "pcc_wtstop_Word2Vec = []\n",
    "\n",
    "for k in k_list:\n",
    "    pcc_raw_Word2Vec.append(cross_validation_ensemble_knn(eval_data, ['Vector_Word2Vec_'+Raw_tweet], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_clean_Word2Vec.append(cross_validation_ensemble_knn(eval_data, ['Vector_Word2Vec_Cleaned_tweet'], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "    pcc_wtstop_Word2Vec.append(cross_validation_ensemble_knn(eval_data, ['Vector_Word2Vec_Cleaned_tweet_wt_stopwords'], 'Class', K_fold, [k], 'labels', 'pcc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest PCC score:  0.2258041909958301  with k =  25\n",
      "The highest PCC score:  0.3286663375433289  with k =  25\n",
      "The highest PCC score:  0.3428537945713763  with k =  13\n"
     ]
    }
   ],
   "source": [
    "for array in [pcc_raw_Word2Vec, pcc_clean_Word2Vec, pcc_wtstop_Word2Vec]:\n",
    "    print('The highest PCC score: ', max(array), ' with k = ', k_list[array.index(max(array))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result for Word2Vec embedding is PCC = 0.3428, for the cleaned tweets without stop words with k=13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate lexicon vectors with wkNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1476203194946398"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_ensemble_knn(eval_data, ['Vector_vad'], 'Class', K_fold, [23], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.126552439618361"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_ensemble_knn(eval_data, ['Vector_emolex'], 'Class', K_fold, [23], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12462609469605984"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_ensemble_knn(eval_data, ['Vector_ai'], 'Class', K_fold, [23], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09334916798515583"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_ensemble_knn(eval_data, ['Vector_anew'], 'Class', K_fold, [23], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06284205672907116"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_ensemble_knn(eval_data, ['Vector_warriner'], 'Class', K_fold, [23], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07168080496100634"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_ensemble_knn(eval_data, ['Vector_all_lexicons'], 'Class', K_fold, [23], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest PCC score:  0.17568084042800533  with k =  25\n"
     ]
    }
   ],
   "source": [
    "# evaluate k for the best lexicon\n",
    "\n",
    "vad_pcc = []\n",
    "\n",
    "for k in k_list:\n",
    "    vad_pcc.append(cross_validation_ensemble_knn(eval_data, ['Vector_vad'], 'Class', K_fold, [k], 'labels', 'pcc'))\n",
    "\n",
    "print('The highest PCC score: ', max(vad_pcc), ' with k = ', k_list[vad_pcc.index(max(vad_pcc))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying to improve the best setup for each embedding vector by adding the lexicons scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roBERTa\n",
      "vad\n",
      "0.6551149880734646\n",
      "\n",
      "\n",
      "roBERTa\n",
      "emolex\n",
      "0.662168758171656\n",
      "\n",
      "\n",
      "roBERTa\n",
      "ai\n",
      "0.6575718740398746\n",
      "\n",
      "\n",
      "roBERTa\n",
      "anew\n",
      "0.6614993743942349\n",
      "\n",
      "\n",
      "roBERTa\n",
      "warriner\n",
      "0.6553673896886737\n",
      "\n",
      "\n",
      "DeepMoji\n",
      "vad\n",
      "0.547434057943307\n",
      "\n",
      "\n",
      "DeepMoji\n",
      "emolex\n",
      "0.5430539013777578\n",
      "\n",
      "\n",
      "DeepMoji\n",
      "ai\n",
      "0.5519381664125546\n",
      "\n",
      "\n",
      "DeepMoji\n",
      "anew\n",
      "0.5633770965541167\n",
      "\n",
      "\n",
      "DeepMoji\n",
      "warriner\n",
      "0.5436415534152078\n",
      "\n",
      "\n",
      "BERT\n",
      "vad\n",
      "0.4139455987667056\n",
      "\n",
      "\n",
      "BERT\n",
      "emolex\n",
      "0.42246833906371617\n",
      "\n",
      "\n",
      "BERT\n",
      "ai\n",
      "0.41516869905507026\n",
      "\n",
      "\n",
      "BERT\n",
      "anew\n",
      "0.4173472930588189\n",
      "\n",
      "\n",
      "BERT\n",
      "warriner\n",
      "0.4127843476346432\n",
      "\n",
      "\n",
      "sBERT\n",
      "vad\n",
      "0.4666241941646674\n",
      "\n",
      "\n",
      "sBERT\n",
      "emolex\n",
      "0.48005716969446083\n",
      "\n",
      "\n",
      "sBERT\n",
      "ai\n",
      "0.47014700882669225\n",
      "\n",
      "\n",
      "sBERT\n",
      "anew\n",
      "0.46705930697530706\n",
      "\n",
      "\n",
      "sBERT\n",
      "warriner\n",
      "0.46894322973699015\n",
      "\n",
      "\n",
      "USE\n",
      "vad\n",
      "0.5197846532915962\n",
      "\n",
      "\n",
      "USE\n",
      "emolex\n",
      "0.4963473805188579\n",
      "\n",
      "\n",
      "USE\n",
      "ai\n",
      "0.5202263728592667\n",
      "\n",
      "\n",
      "USE\n",
      "anew\n",
      "0.5068671231675517\n",
      "\n",
      "\n",
      "USE\n",
      "warriner\n",
      "0.5036098891517351\n",
      "\n",
      "\n",
      "Word2Vec\n",
      "vad\n",
      "0.22998473593066882\n",
      "\n",
      "\n",
      "Word2Vec\n",
      "emolex\n",
      "0.22468338251782569\n",
      "\n",
      "\n",
      "Word2Vec\n",
      "ai\n",
      "0.2276347070325106\n",
      "\n",
      "\n",
      "Word2Vec\n",
      "anew\n",
      "0.20225246364770114\n",
      "\n",
      "\n",
      "Word2Vec\n",
      "warriner\n",
      "0.20538268981321497\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embed in [('roBERTa', 23), ('DeepMoji', 25), ('BERT', 25), ('sBERT', 19), (\"USE\", 21), (\"Word2Vec\", 13)]:\n",
    "    for lex in ['vad', 'emolex', 'ai', 'anew', 'warriner']:\n",
    "        print(embed[0])\n",
    "        print(lex)\n",
    "        print(cross_validation_ensemble_knn(eval_data, ['Vector_'+embed[0]+'_'+lex], \n",
    "                                            'Class', K_fold, [embed[1]], 'labels', 'pcc'))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best setups was impoved only for the **USE** embedding is PCC = 0.5202, for combination with **AI lexicon** (k=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6530347043034405"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all 6 models\n",
    "\n",
    "cross_validation_ensemble_knn(eval_data, ['Vector_roBERTa_'+Raw_tweet,'Vector_DeepMoji_Cleaned_tweet','Vector_BERT_'+Raw_tweet,\n",
    "                                         'Vector_sBERT_'+Raw_tweet,'Vector_USE_Cleaned_tweet_wt_stopwords',\n",
    "                                          'Vector_Word2Vec_Cleaned_tweet_wt_stopwords'], \n",
    "                              \"Class\", K_fold, [23, 25, 35, 19, 21, 13], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6353227386471721"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the best lexicon vector\n",
    "\n",
    "cross_validation_ensemble_knn(eval_data, ['Vector_roBERTa_'+Raw_tweet,'Vector_DeepMoji_Cleaned_tweet','Vector_BERT_'+Raw_tweet,\n",
    "                                         'Vector_sBERT_'+Raw_tweet,'Vector_USE_Cleaned_tweet_wt_stopwords',\n",
    "                                          'Vector_Word2Vec_Cleaned_tweet_wt_stopwords', 'Vector_vad'], \n",
    "                              \"Class\", K_fold, [23, 25, 35, 19, 21, 13, 25], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5296172440623522"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add all lexicons vectors + combination\n",
    "\n",
    "cross_validation_ensemble_knn(eval_data, ['Vector_roBERTa_'+Raw_tweet,'Vector_DeepMoji_Cleaned_tweet','Vector_BERT_'+Raw_tweet,\n",
    "                                         'Vector_sBERT_'+Raw_tweet,'Vector_USE_Cleaned_tweet_wt_stopwords',\n",
    "                                          'Vector_Word2Vec_Cleaned_tweet_wt_stopwords', 'Vector_vad', 'Vector_emolex', \n",
    "                                          'Vector_ai', 'Vector_anew', 'Vector_warriner', 'Vector_all_lexicons'], \n",
    "                              \"Class\", K_fold, [23, 25, 35, 19, 21, 13, 25, 23, 23, 23, 23, 23], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6461547385933937"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with updated by lexicon USE\n",
    "\n",
    "cross_validation_ensemble_knn(eval_data, ['Vector_roBERTa_'+Raw_tweet,'Vector_DeepMoji_Cleaned_tweet','Vector_BERT_'+Raw_tweet,\n",
    "                                     'Vector_sBERT_'+Raw_tweet,'Vector_USE_ai', 'Vector_Word2Vec_Cleaned_tweet_wt_stopwords'], \n",
    "                              \"Class\", K_fold, [23, 25, 35, 19, 21, 13], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6395912047460274"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with updated by lexicon USE + the best lexicon\n",
    "\n",
    "cross_validation_ensemble_knn(eval_data, ['Vector_roBERTa_'+Raw_tweet,'Vector_DeepMoji_Cleaned_tweet','Vector_BERT_'+Raw_tweet,\n",
    "                                     'Vector_sBERT_'+Raw_tweet,'Vector_USE_ai', 'Vector_Word2Vec_Cleaned_tweet_wt_stopwords', \n",
    "                                         'Vector_vad'], \"Class\", K_fold, [23, 25, 35, 19, 21, 13, 25], 'labels', 'pcc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best ensemble for anger is based on all embeddings (PCC=0.65), but it still worse than single roBERTa model, which is the best setup (PCC=0.66)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the best method on the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply embeddings on the test data\n",
    "\n",
    "test_data['Vector_roBERTa_'+Raw_tweet] = test_data[Raw_tweet].apply(lambda x: \n",
    "                                            get_vector_roberta(x, tokenizer_roberta, model_roberta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted labels\n",
    "\n",
    "test_labels = knn_ensemble_labels(eval_data, eval_data['Class'], test_data, ['Vector_roBERTa_'+Raw_tweet], [23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6737452388532557"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(test_data['Class'].to_list(), test_labels)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore explainability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of wrong predicted test instances\n",
    "\n",
    "error = [i for i in range(len(test_data['Class'])) if test_data['Class'][i] != test_labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 8, 10, 11, 16, 17, 18, 20, 22]\n"
     ]
    }
   ],
   "source": [
    "print(error[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@PageShhh1 I know you mean well but I'm offended. Prick.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Correct prediction sample\n",
    "\n",
    "i = 0 # not in error list\n",
    "print(test_data['Tweet'].iloc[i])\n",
    "print(test_data['Class'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['@Jack_Septic_Eye Grass growing simulator is offended',\n",
       "  \"@healeyraine I'm offended, I actually am\",\n",
       "  'You offend me, @Tansorma',\n",
       "  '@Idubbbz @LeafyIsHere  I am offended',\n",
       "  \"@NeoFundie @fitchest Ha. Right. I'm from San Jose, CA, and I was offended right there with you. Dave, go on a walk or something next time.\"],\n",
       " [1, 2, 2, 2, 3])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore its train neighbours and their classes\n",
    "\n",
    "test_vector = test_data['Vector_roBERTa_'+Raw_tweet].iloc[i]\n",
    "get_neigbours(test_vector, eval_data, 'Vector_roBERTa_'+Raw_tweet, 5, 'Tweet', 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ajduey04303 We've been broken up a while, both moved on, she's got a kid, I don't hold any animosity towards her anymore...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Wrong prediction sample\n",
    "\n",
    "i = error[1] # not in error list\n",
    "print(test_data['Tweet'].iloc[i])\n",
    "print(test_data['Class'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"I told myself i wouldn't talk about this but i need to bring it up. I'm slightly bitter about the tÃ¸p cover of cancer\",\n",
       "  '@Iucifaer you can go on what you usually do its just their own personal reason and not mean to offend anyone :(',\n",
       "  \"@StarklyDark 'I know you trusted me.' His words were soft as he ignored the anger and focused on the hurt beneath. 'I know I screwed up.' --\",\n",
       "  '@BoJackHorseman if your depressed and somebody calls you long faced will you still automatically take umbrage?',\n",
       "  '@bassekraah. @sunilddesai @jyoti1013 @Archnahr . A2: #irritation and isolation #lifenabler'],\n",
       " [2, 3, 2, 0, 2])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore its train neighbours and their classes\n",
    "\n",
    "test_vector = test_data['Vector_roBERTa_'+Raw_tweet].iloc[i]\n",
    "get_neigbours(test_vector, eval_data, 'Vector_roBERTa_'+Raw_tweet, 5, 'Tweet', 'Class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
